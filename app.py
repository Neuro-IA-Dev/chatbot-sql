
import os
import json
import numpy as np
import datetime
import requests
import streamlit as st
import mysql.connector
import pandas as pd
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from openai import OpenAI
import re

# CONFIG STREAMLIT
st.set_page_config(page_title="Asistente Inteligente de NeuroVIA", page_icon="ğŸ§ ")
st.image("assets/logo_neurovia.png", width=180)
st.title(":brain: Asistente Inteligente de Intanis/NeuroVIA")

if "historial" not in st.session_state:
    st.session_state["historial"] = []
if "conversacion" not in st.session_state:
    st.session_state["conversacion"] = []
if "contexto" not in st.session_state:
    st.session_state["contexto"] = {}

if st.button("ğŸ§¹ Borrar historial de preguntas", key="btn_borrar_historial"):
    st.session_state["historial"] = []
    st.session_state["conversacion"] = []
    st.success("Historial de conversaciÃ³n borrado.")

st.markdown("Haz una pregunta y el sistema generarÃ¡ y ejecutarÃ¡ una consulta SQL automÃ¡ticamente.")

llm = ChatOpenAI(temperature=0)

def connect_db():
    return mysql.connector.connect(
        host="s1355.use1.mysecurecloudhost.com",
        port=3306,
        user="domolabs_RedTabBot_USER",
        password="Pa$$w0rd_123",
        database="domolabs_RedTabBot_DB"
    )

def es_consulta_segura(sql):
    sql = sql.lower()
    comandos_peligrosos = ["drop", "delete", "truncate", "alter", "update", "insert", "--", "/*", "grant", "revoke"]
    return not any(comando in sql for comando in comandos_peligrosos)

sql_prompt = PromptTemplate(
    input_variables=["pregunta"],
    template="""
1. Si el usuario menciona tÃ©rminos como "tienda", "cliente", "marca", "canal", "producto", "temporada", "calidad", etc., asume que se refiere a su campo descriptivo (DESC_...) y **no al cÃ³digo (COD_...)**, excepto que el usuario especifique explÃ­citamente â€œcÃ³digo de...â€.

   - Ejemplo: "tienda" â†’ DESC_TIENDA
   - Ejemplo: "marca" â†’ DESC_MARCA
   - Ejemplo: "calidad" â†’ DESC_CALIDAD
   - Ejemplo: "temporada" â†’ DESC_TEMPORADA
   - Ejemplo: "producto" â†’ DESC_ARTICULO
   - Ejemplo: "cÃ³digo de tienda" â†’ COD_TIENDA

   Cuando el usuario mencione palabras que parecen referirse a nombres de marcas o productos (por ejemplo: "Levis", "Nike", "Adidas", etc.), **bÃºscalas en DESC_MARCA**.

   Cuando el usuario mencione nombres de ciudades, centros comerciales u otros lugares (por ejemplo: "Costanera", "Talca", "Plaza Vespucio"), **bÃºscalos en DESC_TIENDA**.

   Cuando filtres por estos campos descriptivos (DESC_...), usa SIEMPRE la clÃ¡usula LIKE '%valor%' en lugar de =, para permitir coincidencias parciales o mayÃºsculas/minÃºsculas.

2. Si el usuario pide:
   - "Â¿CuÃ¡ntas tiendas?" o "total de tiendas": usa COUNT(DISTINCT DESC_TIENDA)
   - "Â¿CuÃ¡ntos canales?" â†’ COUNT(DISTINCT DESC_CANAL)
   - "Â¿CuÃ¡ntos clientes?" â†’ COUNT(DISTINCT NOMBRE_CLIENTE)

3. Siempre que se mencione:
   - "ventas", "ingresos": usar la columna INGRESOS
   - "costos": usar COSTOS
   - "unidades vendidas": usar UNIDADES
   - "producto", "artÃ­culo", "sku": puedes usar DESC_ARTICULO o DESC_SKU dependiendo del contexto.

4. No asumas que hay relaciones externas: toda la informaciÃ³n estÃ¡ embebida en el tablon VENTAS.

5. Cuando pregunten por montos como ingresos o ventas, consulta si la informaciÃ³n requerida debe ser en CLP o USD. Esta informaciÃ³n estÃ¡ disponible en la columna MONEDA.

6. Cuando pregunten algo como "muestrame el codigo y descripcion de todas las tiendas que hay" debes hacer un distinct.

7. "Despacho a domicilio" es un ARTICULO

8. Fecha de venta es FECHA_DOCUMENTO.

9.- Si se menciona â€œpara mujerâ€, â€œde mujerâ€, â€œfemeninoâ€ o â€œde damaâ€, filtra con DESC_GENERO LIKE '%woman%'.
- Si se menciona â€œpara hombreâ€, â€œmasculinoâ€, â€œde varÃ³nâ€ o â€œde caballeroâ€, filtra con DESC_GENERO LIKE '%men%'.
- Si se menciona â€œunisexâ€, usa DESC_GENERO LIKE '%unisex%'.

10. Siempre que se pregunte "Â¿de quÃ© canal es esa tienda?", "Â¿quÃ© canal pertenece?" o algo similar, usa `SELECT DISTINCT DESC_CANAL ...` para evitar resultados duplicados.

Cuando se reemplace un valor como â€œese artÃ­culoâ€, â€œesa tiendaâ€, etc., asegÃºrate de utilizar siempre `LIKE '%valor%'` en lugar de `=` para evitar errores por coincidencias exactas.

ğŸ” Recuerda usar WHERE, GROUP BY o ORDER BY cuando el usuario pregunte por filtros, agrupaciones o rankings.

ğŸ–ï¸ Cuando generes la consulta SQL, no expliques la respuesta â€”solo entrega el SQL limpio y optimizado para MySQL.

Pregunta: {pregunta}
"""
)

referencias = {
    "esa tienda": "DESC_TIENDA",
    "ese canal": "DESC_CANAL",
    "esa marca": "DESC_MARCA",
    "ese producto": "DESC_ARTICULO",
    "ese artÃ­culo": "DESC_ARTICULO",
    "esa categorÃ­a": "DESC_CATEGORIA",
    "ese cliente": "NOMBRE_CLIENTE"
}

def aplicar_contexto(pregunta):
    pregunta_modificada = pregunta
    for ref, campo in referencias.items():
        if ref.lower() in pregunta.lower() and campo in st.session_state["contexto"]:
            valor_contexto = st.session_state["contexto"][campo]
            pregunta_modificada = re.sub(ref, valor_contexto, pregunta_modificada, flags=re.IGNORECASE)
    return pregunta_modificada

campos_contexto = ["DESC_TIENDA", "DESC_CANAL", "DESC_MARCA", "DESC_ARTICULO", "DESC_GENERO", "NOMBRE_CLIENTE"]

def actualizar_contexto(df):
    for campo in campos_contexto:
        if campo in df.columns and not df[campo].isnull().all():
            st.session_state["contexto"][campo] = str(df[campo].iloc[0])

def log_interaction(pregunta, sql, resultado, feedback=None):
    try:
        conn = connect_db()
        cursor = conn.cursor()
        query = """
        INSERT INTO chat_logs (pregunta, sql_generado, resultado, feedback) 
        VALUES (%s, %s, %s, %s)
        """
        cursor.execute(query, (pregunta, sql, resultado, feedback))
        conn.commit()
        cursor.close()
        conn.close()
    except Exception as e:
        st.warning(f"âš ï¸ No se pudo guardar el log: {e}")

client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

def obtener_embedding(texto):
    try:
        response = client.embeddings.create(input=[texto], model="text-embedding-3-small")
        return response.data[0].embedding
    except:
        return None

def guardar_en_cache(pregunta, sql_generado, embedding):
    try:
        conn = connect_db()
        cursor = conn.cursor()
        cursor.execute(
            "INSERT INTO semantic_cache (pregunta, embedding, sql_generado) VALUES (%s, %s, %s)",
            (pregunta, json.dumps(embedding), sql_generado)
        )
        conn.commit()
        cursor.close()
        conn.close()
    except Exception as e:
        st.warning(f"âŒ No se guardÃ³ en semantic_cache: {e}")

def buscar_sql_en_cache(pregunta_nueva, umbral_similitud=0.90):
    embedding_nuevo = obtener_embedding(pregunta_nueva)
    if embedding_nuevo is None:
        return None

    try:
        conn = connect_db()
        cursor = conn.cursor(dictionary=True)
        cursor.execute("SELECT pregunta, embedding, sql_generado FROM semantic_cache")
        rows = cursor.fetchall()
        cursor.close()
        conn.close()

        vec_nuevo = np.array(embedding_nuevo)
        for row in rows:
            vec_guardado = np.array(json.loads(row["embedding"]))
            similitud = np.dot(vec_nuevo, vec_guardado) / (np.linalg.norm(vec_nuevo) * np.linalg.norm(vec_guardado))
            if similitud >= umbral_similitud:
                return row["sql_generado"]
    except Exception as e:
        st.warning(f"âŒ Error buscando en cache: {e}")
    return None
# ENTRADA DEL USUARIO
pregunta = st.chat_input("ğŸ§  Pregunta en lenguaje natural")

if pregunta:
    with st.chat_message("user"):
        st.markdown(pregunta)

    sql_query = buscar_sql_en_cache(pregunta)
    guardar_en_cache_pending = None

    if sql_query:
        st.info("ğŸ” Consulta reutilizada desde la cache.")
    else:
        pregunta_con_contexto = aplicar_contexto(pregunta)
        prompt_text = sql_prompt.format(pregunta=pregunta_con_contexto)
        sql_query = llm.predict(prompt_text).replace("```sql", "").replace("```", "").strip()
        embedding = obtener_embedding(pregunta)
        guardar_en_cache_pending = embedding if embedding else None

    resultado = ""

    try:
        if not es_consulta_segura(sql_query):
            st.error("âŒ Consulta peligrosa bloqueada.")
            resultado = "Consulta bloqueada"
        else:
            conn = connect_db()
            cursor = conn.cursor()
            cursor.execute(sql_query)

            if sql_query.lower().startswith("select"):
                rows = cursor.fetchall()
                if cursor.description:
                    columns = [col[0] for col in cursor.description]
                    df = pd.DataFrame(rows, columns=columns)
                    if "FECHA_DOCUMENTO" in df.columns:
                        df["FECHA_DOCUMENTO"] = pd.to_datetime(df["FECHA_DOCUMENTO"].astype(str), format="%Y%m%d").dt.strftime("%d/%m/%Y")

                    st.dataframe(df)
                    resultado = f"{len(df)} filas"
                    actualizar_contexto(df)
                else:
                    resultado = "La consulta no devolviÃ³ resultados."
            else:
                conn.commit()
                resultado = "Consulta ejecutada."

            cursor.close()
            conn.close()

    except Exception as e:
        resultado = f"âŒ Error ejecutando SQL: {e}"

    st.session_state["conversacion"].append({
        "pregunta": pregunta,
        "respuesta": resultado,
        "sql": sql_query,
        "cache": guardar_en_cache_pending
    })

# MOSTRAR TODAS LAS INTERACCIONES COMO CHAT
# UI MEJORADA EN STREAMLIT
# (Esta parte va justo al final del archivo app.py, reemplazando el bloque de visualizaciÃ³n actual de interacciones)

if pregunta:
    with st.chat_message("user"):
        st.markdown(f"### ğŸ¤– Pregunta actual:")
        st.markdown(f"> {pregunta}")

    with st.chat_message("assistant"):
        st.markdown("### ğŸ” Consulta SQL Generada:")
        st.code(sql_query, language="sql")
        st.markdown("### ğŸ’¬ Respuesta:")
        st.markdown(resultado)

        col1, col2 = st.columns(2)
        with col1:
            if st.button("âœ… Fue acertada", key=f"ok_last"):
                st.success("Gracias por tu feedback. ğŸ‘")
                if guardar_en_cache_pending:
                    guardar_en_cache(pregunta, sql_query, guardar_en_cache_pending)
                log_interaction(pregunta, sql_query, resultado, "acertada")
        with col2:
            if st.button("âŒ No fue correcta", key=f"fail_last"):
                st.warning("Gracias por reportarlo. Mejoraremos esta consulta. ğŸš²")
                log_interaction(pregunta, sql_query, resultado, "incorrecta")

    st.markdown("---")

# MOSTRAR HISTORIAL PREVIO (EXCLUYENDO LA Ãšltima PREGUNTA)
if st.session_state["conversacion"]:
    st.markdown("## âŒ› Historial de preguntas anteriores")
    for i, item in enumerate(reversed(st.session_state["conversacion"][:-1])):
        with st.expander(f"ğŸ’¬ {item['pregunta']}", expanded=False):
            st.markdown("**Consulta SQL Generada:**")
            st.code(item["sql"], language="sql")
            st.markdown(f"**Respuesta:** {item['respuesta']}")

            col1, col2 = st.columns(2)
            with col1:
                if st.button("âœ… Fue acertada", key=f"ok_{i}"):
                    st.success("Gracias por tu feedback. ğŸ‘")
                    if item.get("cache"):
                        guardar_en_cache(item["pregunta"], item["sql"], item["cache"])
                    log_interaction(item["pregunta"], item["sql"], item["respuesta"], "acertada")
            with col2:
                if st.button("âŒ No fue correcta", key=f"fail_{i}"):
                    st.warning("Gracias por reportarlo. Mejoraremos esta consulta. ğŸš²")
                    log_interaction(item["pregunta"], item["sql"], item["respuesta"], "incorrecta")


        st.markdown("---")
